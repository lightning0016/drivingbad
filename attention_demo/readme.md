最容易理解的attention模型,使用了Keras和Tensorflow

输入x为10维向量,其中9个是1,随机一个位置是0-100000的数 并且与输出相同

通过看attention_layer的结果:
![avatar](attention.jpg)
随机数对应的位置的attention是1
